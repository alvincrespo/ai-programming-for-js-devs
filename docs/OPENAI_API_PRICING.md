# OpenAI API Pricing Overview

This document explains how OpenAI API pricing works for models like `gpt-5`.

---

## Pricing Table

| Model | Input | Cached Input | Output |
|--------|--------|---------------|---------|
| gpt-5 | $1.25 / 1M tokens | $0.125 / 1M tokens | $10.00 / 1M tokens |

---

## Definitions

### Model
The name of the OpenAI model being used (e.g., `gpt-5`, `gpt-4.1`, etc.).
Each model has different capabilities, speeds, and costs.

### Input
Price per **1 million tokens** sent to the model in your request (system prompt, user input, context, etc.).

Example:
```
100,000 input tokens → 100,000 / 1,000,000 × $1.25 = $0.125
```

This is the price you pay for new tokens that you send to the model in your request (the “prompt” or “context” you provide). For example, when you send 100,000 tokens of text to the API, you would pay ~100 k / 1 000 000 × $1.25 = $0.125 for those tokens (if using that model rate).

In other words: how much it costs to send data into the model.

Tokens are roughly words/pieces of words.

### Cached Input
Discounted rate for tokens that have been **previously processed** and are **reused** via caching.
Useful when your app repeatedly sends the same long prompt or shared context.

Example:
```
80,000 cached tokens → 80,000 / 1,000,000 × $0.125 = $0.010
```

This is a discount rate for input tokens that the system recognises as repeating / already processed and hence doesn’t need full processing again. In effect, if you send a long prompt and then send similar/identical prefixes in subsequent requests, those repeated tokens can qualify for “cached” billing instead of full “input” billing.

So if you have a prompt of, say, 2,000 tokens, and the first 1,200 are identical to a previous call and have been cached, then only the non-cached part is billed at full “Input” rate, and the cached part is billed at the “Cached input” rate.

Note: Cached input tokens cost significantly less (in your example, $0.125 vs $1.25 per 1 M tokens).

This mechanism is described in their docs (and also in Azure’s docs for prompt caching) as a way to reduce cost when a lot of the input is repetitive.

### Output
Price per **1 million tokens** generated by the model.
Output tokens are typically more expensive because generation requires more computation.

Example:
```
20,000 output tokens → 20,000 / 1,000,000 × $10.00 = $0.20
```

This is the price you pay for tokens generated by the model in its response. So if the model returns 50,000 tokens in output, you would pay 50 k / 1 000 000 × $10.00 = $0.50 in the example for GPT-5.

In other words: how much it costs to get model output back.

Because output typically involves more computation, output tokens often have a higher price than input tokens.

---


Putting it together

So if you send a prompt (input) of X tokens, some of which are recognized as cached (Y tokens) and the rest are new (X–Y tokens), and you get a reply of Z tokens (output), your cost roughly is:

```
Cost=((X–Y)×Input rate+Y×Cached Input rate+Z×Output rate)
```

Where rates are in “$ per 1 million tokens”.


### Why “cached input” exists & how it works

If you make multiple API calls with the same prefix of text (e.g., the same long context or conversation history) the system can avoid re-processing that part. In effect, it’s already “done the work” once, so you pay less next time.

According to documentation (Azure’s version), for prompt-caching to apply, the initial prefix must meet a minimum length (e.g., 1,024 tokens) and match exactly the previous prompt.

Because of this, if you’re repeatedly sending the same large context + varying small new user input each time, you can significantly reduce cost by re-using (via cache) the large static context portion.

---

## Cost Example

| Type | Tokens | Rate | Cost |
|------|--------:|------:|------:|
| Input | 20,000 | $1.25 / 1M | $0.025 |
| Cached Input | 80,000 | $0.125 / 1M | $0.010 |
| Output | 20,000 | $10.00 / 1M | $0.200 |
| **Total** | — | — | **$0.235** |

If there were no cached tokens, the total cost would be **$0.325** instead.

---

## Formula

```
Total Cost =
  (New Input Tokens × Input Rate) +
  (Cached Tokens × Cached Rate) +
  (Output Tokens × Output Rate)
```

All rates are per **1 million tokens**.

---

## Best Practices

- Track input/output token counts in API responses.
- Reuse long static prompts to benefit from caching.
- Limit response length to reduce cost.
- Use [tiktoken](https://github.com/openai/tiktoken) to estimate tokens before sending requests.

---

## References

- [OpenAI Pricing Documentation](https://platform.openai.com/docs/pricing)
- [Prompt Caching Guide](https://platform.openai.com/docs/guides/prompt-caching)
- [What Are Tokens?](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)
